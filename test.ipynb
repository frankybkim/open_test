
# Define input layer
inputs = layers.Input(shape=(max_sequence_length,))

# Define embedding layer
embedding_layer = layers.Embedding(
    input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length
)(inputs)

# Define transformer layers
num_transformer_layers = 12
transformer_layers = []
for i in range(num_transformer_layers):
    transformer_layers.append(
        layers.Transformer(
            num_heads=num_heads,
            ff_dim=feedforward_dim,
            dropout=dropout_rate,
            activation=activation_function
        )
    )

# Apply transformer layers in a stack
transformer_output = embedding_layer
for transformer_layer in transformer_layers:
    transformer_output = transformer_layer(transformer_output)

# Define output layer
outputs = layers.Dense(vocab_size, activation='softmax')(transformer_output)

# Define GPT model
model = keras.Model(inputs=inputs, outputs=outputs)

# Compile the model
model.compile(optimizer=optimizer, loss=loss_function, metrics=metric_function)
